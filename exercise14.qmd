---
title: "Homework Assignment 6"
author: "Vincent Xie"
toc: true
number-sections: true
highlight-style: pygments
format: 
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
---

# Logistic modeling for response time over 3 days {.unnumbered}

The response time to 311 service requests is a measure of civic
service quality. Let us model the response time to 311 requests
with complain type `Rodent`.

**Data Initialization**
```{python}
import pandas as pd

file_path = 'data/rodent_2022-2023.csv'
data = pd.read_csv(file_path)
```

1. **Compute the response time in hours. Note that some response**
   **will be missing because of unavailable closed date.**

```{python}
from datetime import datetime

# Convert to datetime   
data['Created Date'] = pd.to_datetime(data['Created Date'])
data['Closed Date'] = pd.to_datetime(data['Closed Date'])

# Filter out records where 'Closed Date' is missing
data = data[data['Closed Date'].notna()]

# Calculate Response Time
data['Response Time'] = (data['Closed Date'] - data['Created Date']).dt.total_seconds() / 3600
print(data['Response Time'].head())
```

2. **Compute a binary variable `over3d`, which is one if the**
   **response time is greater than 3 days, and zero otherwise. Note**
   **that this variable should have no missing values.**

```{python}
# Create the binary variable 'over3d'
data['over3d'] = (data['Response Time'] > 72).astype(int)
```

3. **Use the package `uszipcode` to obtain the zip code level**
   **covaraites such as median house income and median home value.**
   **Merge these variables to the rodent data.**

```{python}
from uszipcode import SearchEngine

# Initialize SearchEngine
search = SearchEngine()

def zipcode_data(zip_code):
    zipcode = search.by_zipcode(zip_code)
    if zipcode:
        return pd.Series([zipcode.median_household_income, zipcode.median_home_value],
                         index=['median_house_income', 'median_home_value'])
    else:
        return pd.Series([None, None], index=['median_house_income', 'median_home_value'])

# Apply function to data
data[['median_house_income', 'median_home_value']] = data['Incident Zip'].apply(zipcode_data)
```

4. **Split the data at random into training (80%) and testing (20%).**
   **Build a logistic model to predict `over3d` on the training data,**
   **and validate the performance on the testing data.**

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
import warnings
from sklearn.exceptions import UndefinedMetricWarning
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Suppress only the UndefinedMetricWarning
warnings.filterwarnings("ignore", category=UndefinedMetricWarning)

X = data[['median_house_income', 'median_home_value']] 
y = data['over3d']

# Impute missing values with median
imputer = SimpleImputer(strategy='median')

# Create a pipeline with imputation and logistic regression
log_model = make_pipeline(imputer, LogisticRegression())

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the model
log_model.fit(X_train, y_train)

# Predictions
y_pred = log_model.predict(X_test)
y_pred_proba = log_model.predict_proba(X_test)[:, 1]

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Classification Report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix)

# ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr, tpr, label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()
```

**Analysis:** 

- 86% of the predictions match the true label.
- In the classification report, the positive class has 0 for both precision and recall. This indicates that the model did not correctly identify any positive instances.
- Confusion matrix: 13,766 true negatives and 2,251 false negatives.
- AUC = 0.53 which is only slightly better than 0.5. The model does not have a good discriminative ability between positive and negative classes.

5. **Build a lasso logistic model to predict `over3d`, and justify**
   **your choice of the tuning parameter. Validate on the testing data.**

```{python}
# Impute missing values with median
imputer = SimpleImputer(strategy='median')

# Create a pipeline with imputation and Lasso Logistic Regression
lasso_log_model = make_pipeline(imputer, LogisticRegressionCV(cv=5, penalty='l1', 
solver='saga', max_iter=1000, random_state=0))

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit the Lasso Logistic model
lasso_log_model.fit(X_train, y_train)

# Using the Lasso Logistic Model for predictions
y_pred_lasso = lasso_log_model.predict(X_test)
y_pred_proba_lasso = lasso_log_model.predict_proba(X_test)[:, 1]

# Accuracy
accuracy_lasso = accuracy_score(y_test, y_pred_lasso)
print("Accuracy (Lasso):", accuracy_lasso)

# Classification Report
print("\nClassification Report (Lasso):\n", classification_report(y_test, y_pred_lasso))

# Confusion Matrix
conf_matrix_lasso = confusion_matrix(y_test, y_pred_lasso)
print("\nConfusion Matrix (Lasso):\n", conf_matrix_lasso)

# ROC Curve and AUC
fpr_lasso, tpr_lasso, _ = roc_curve(y_test, y_pred_proba_lasso)
auc_lasso = roc_auc_score(y_test, y_pred_proba_lasso)
plt.plot(fpr_lasso, tpr_lasso, label="AUC (Lasso)="+str(auc_lasso))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()
```

**Analysis:** 

- 86% of the predictions match the true label so it is good at predicting the negative class
- In the classification report, the positive class has 0 for both precision and recall. This indicates that the model did not correctly identify any positive instances.
- Confusion matrix: 13,766 true negatives and 2,251 false negatives.
- There are no true positives and false positives.
- AUC = 0.44 which is less than 0.5. This suggests that the model is no better than random guessing.